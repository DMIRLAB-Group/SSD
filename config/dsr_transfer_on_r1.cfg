[model]
learning_rate = 0.001
dropout=  0.5
word_dropout = 0.5
filter_size = 3
pre_word_emb_path=./data/glove/word.100d.embedding.npz
word_vocab_size = 48080
word_emb_dim = 100
char_emb_dim = 100
char_vocab_size = 84
num_filter = 30
bilstm_hidden_size = 200
word_max_length = 20
sent_max_length = 60
optimizer = adam
source_tag_class = 23
target_tag_class = 21
lamta = 0.005
beta = 0.01
gamma = 0.01
sigema = 0.01
pre_train_path = ./model/pre_dsr/on_r1_1/best_model.ckpt

[run]
title = dsr_on_r1
tensorboard_log = /home/lvdi/HDD/lvdi/model/on_r1
gpu = 1
best_model_path = /home/lvdi/HDD/lvdi/model/dsr/on_r1/best_model.ckpt
batch_size = 64
step = 20000
word_vocab = ./data/share_vocab/word.vocab
char_vocab = ./data/share_vocab/on_r1_char.vocab
source_tag_vocab = ./data/ontonote-nw/tag.vocab
target_tag_vocab = ./data/ritter2011/tag.vocab
source_train_path =  ./data/ontonote-nw/train/train
source_dev_path =  ./data/ontonote-nw/dev/dev
source_test_path =  ./data/ontonote-nw/test/test
target_train_path =  ./data/ritter2011/train/train
target_dev_path =  ./data/ritter2011/dev/dev
target_test_path =  ./data/ritter2011/test/test
result_file = /home/lvdi/HDD/lvdi/ner/result/dsr/on_r1/result.txt
flag = file
log_file = /home/lvdi/HDD/lvdi/ner/result/dsr/on_r1/log.txt
